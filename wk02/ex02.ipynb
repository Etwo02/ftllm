{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPAug imported successfully\n",
      "Torch version: 2.5.1+cu121\n",
      "Transformers version: 4.48.1\n",
      "CUDA available: True\n",
      "Current device: NVIDIA GeForce RTX 4080 SUPER\n",
      "cuda\n",
      "Original reviews: 500\n",
      "Augmented reviews: 1362\n",
      "Total reviews: 1862\n",
      "\n",
      "Example augmentations:\n",
      "\n",
      "Original: Outstanding performance.\n",
      "Augmented: not what i originally expected at all.\n",
      "\n",
      "Original: Honestly, It's the best I've ever used.\n",
      "Augmented: fortunately, great customer service.\n",
      "\n",
      "Original: Honestly, Exactly what I was looking for.\n",
      "Augmented: Honestly, This is amazing!\n"
     ]
    }
   ],
   "source": [
    "# Category 1, Dataset Preparation \n",
    "\n",
    "# 1. Synthetic Dataset Creation and Augmentation \n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import nlpaug.augmenter.word as naw\n",
    "import torch\n",
    "print(\"NLPAug imported successfully\")\n",
    "import random\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Sample positive and negative reviews\n",
    "positive_reviews = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I highly recommend this.\",\n",
    "    \"It's the best I've ever used.\",\n",
    "    \"Excellent quality and value.\",\n",
    "    \"Five stars!\",\n",
    "    \"Great customer service.\",\n",
    "    \"Exactly what I was looking for.\",\n",
    "    \"Very satisfied with my purchase.\",\n",
    "    \"Outstanding performance.\",\n",
    "    \"Worth every penny!\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This product is terrible.\",\n",
    "    \"I would not recommend this.\",\n",
    "    \"It's the worst I've ever used.\",\n",
    "    \"Poor quality and overpriced.\",\n",
    "    \"One star!\",\n",
    "    \"Horrible customer service.\",\n",
    "    \"Not what I expected at all.\",\n",
    "    \"Very disappointed with my purchase.\",\n",
    "    \"Unreliable performance.\",\n",
    "    \"Complete waste of money!\"\n",
    "]\n",
    "\n",
    "# Generate 500 base sentences by repeating and slightly modifying the samples\n",
    "all_reviews = []\n",
    "for i in range(250):\n",
    "    # Add some random variation to avoid exact duplicates\n",
    "    pos_review = positive_reviews[i % len(positive_reviews)]\n",
    "    neg_review = negative_reviews[i % len(negative_reviews)]\n",
    "\n",
    "    # Add simple variations to make the dataset more diverse\n",
    "    if random.random() > 0.5:\n",
    "        pos_review = \"Honestly, \" + pos_review\n",
    "    if random.random() > 0.5:\n",
    "        neg_review = \"Unfortunately, \" + neg_review\n",
    "\n",
    "    all_reviews.append(pos_review)\n",
    "    all_reviews.append(neg_review)\n",
    "\n",
    "# Initialize augmenters\n",
    "aug_insert = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"insert\",\n",
    "    aug_p=0.1  # Probability of augmenting each word\n",
    ")\n",
    "\n",
    "aug_sub = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.1,\n",
    "    stopwords=['not', 'no', 'never']  # Prevent changing sentiment-critical words\n",
    ")\n",
    "\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_reviews = []\n",
    "for review in all_reviews:\n",
    "    try:\n",
    "        # Insert words\n",
    "        aug_text = aug_insert.augment(review)[0]\n",
    "        augmented_reviews.append(aug_text)\n",
    "\n",
    "        # Substitute words\n",
    "        aug_text = aug_sub.augment(review)[0]\n",
    "        augmented_reviews.append(aug_text)\n",
    "\n",
    "        # Simple word deletion (manual approach)\n",
    "        words = review.split()\n",
    "        if len(words) > 3:  # Only delete if we have enough words\n",
    "            del_idx = random.randint(0, len(words)-1)\n",
    "            words.pop(del_idx)\n",
    "            aug_text = \" \".join(words)\n",
    "            augmented_reviews.append(aug_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting review: {review}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine original and augmented reviews\n",
    "final_reviews = all_reviews + augmented_reviews\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Original reviews: {len(all_reviews)}\")\n",
    "print(f\"Augmented reviews: {len(augmented_reviews)}\")\n",
    "print(f\"Total reviews: {len(final_reviews)}\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\nExample augmentations:\")\n",
    "for i in range(3):\n",
    "    orig_idx = random.randint(0, len(all_reviews)-1)\n",
    "    aug_idx = random.randint(0, len(augmented_reviews)-1)\n",
    "    print(f\"\\nOriginal: {all_reviews[orig_idx]}\")\n",
    "    print(f\"Augmented: {augmented_reviews[aug_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total reviews: 1867\n",
      "Gaps created: 186\n",
      "\n",
      "Reconstruction Examples:\n",
      "Reconstructed: a total  this product is just terrible. this product screams terrible.\n",
      "Reconstructed: review poor work and overpriced. Poor and over priced.\n",
      "Reconstructed: this product line is terrible. this product appeared terrible.  honestly, i personally recommend this.\n",
      "Reconstructed: Sadly, Very disappointed with my purchase.  Unfortunately, Unreliable performance.\n",
      "Reconstructed: a complete  Unfortunately, Not what I expected at all.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Handling Missing Values\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ReviewReconstructor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.generator = self._create_generator()\n",
    "        # Sentiment keywords for better context understanding\n",
    "        self.positive_keywords = {'amazing', 'recommend', 'best', 'excellent', 'stars', 'great', \n",
    "                                'exactly', 'satisfied', 'outstanding', 'worth'}\n",
    "        self.negative_keywords = {'terrible', 'not', 'worst', 'poor', 'horrible', 'disappointed', \n",
    "                                'unreliable', 'waste', 'unfortunately'}\n",
    "\n",
    "    def _create_generator(self):\n",
    "        return pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=\"t5-base\",\n",
    "            device=0 if self.device == 'cuda' else -1,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "    def _detect_sentiment(self, text):\n",
    "        \"\"\"Detect sentiment based on keyword presence.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        pos_count = sum(1 for word in self.positive_keywords if word in text_lower)\n",
    "        neg_count = sum(1 for word in self.negative_keywords if word in text_lower)\n",
    "        return 'positive' if pos_count > neg_count else 'negative'\n",
    "\n",
    "    def _get_context(self, text_list, current_idx):\n",
    "        # Get surrounding context\n",
    "        prev_texts = [t for t in text_list[max(0, current_idx - 2):current_idx] if t.strip()]\n",
    "        next_texts = [t for t in text_list[current_idx + 1:current_idx + 3] if t.strip()]\n",
    "        \n",
    "        # Combine context\n",
    "        context_text = \" \".join(prev_texts + next_texts)\n",
    "        sentiment = self._detect_sentiment(context_text)\n",
    "        \n",
    "        # Format prompt with sentiment guidance\n",
    "        prompt = f\"complete {sentiment} review:\"\n",
    "        if prev_texts:\n",
    "            prompt += f\" {' '.join(prev_texts)}\"\n",
    "        prompt += \" [MISSING]\"\n",
    "        if next_texts:\n",
    "            prompt += f\" {' '.join(next_texts)}\"\n",
    "            \n",
    "        return prompt, sentiment\n",
    "\n",
    "    def _clean_generated_text(self, text, sentiment):\n",
    "        \"\"\"Clean and validate generated text.\"\"\"\n",
    "        # Remove common prefix artifacts\n",
    "        artifacts = [\n",
    "            \"review::\", \"negative review:\", \"complete positive review:\",\n",
    "            \"complete negative review:\", \"positive review:\", \"complete review:\", \":\", \"True\"\n",
    "        ]\n",
    "        for artifact in artifacts:\n",
    "            text = text.replace(artifact, \"\").strip()\n",
    "\n",
    "        # Remove [MISSING] placeholders\n",
    "        text = text.replace(\"[MISSING]\", \"\").strip()\n",
    "\n",
    "        # Ensure proper sentence structure\n",
    "        if len(text.split()) < 3:\n",
    "            text = \"This product is excellent!\" if sentiment == 'positive' else \"This product is disappointing.\"\n",
    "\n",
    "        # Ensure proper ending punctuation\n",
    "        if not any(text.endswith(char) for char in \".!?\"):\n",
    "            text += \".\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    def reconstruct_texts(self, reviews, missing_indices, batch_size=32):\n",
    "        \"\"\"Reconstruct missing texts with batched processing.\"\"\"\n",
    "        reconstructed = reviews.copy()\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(missing_indices), batch_size)):\n",
    "            batch_indices = missing_indices[i:i + batch_size]\n",
    "            prompts = []\n",
    "            sentiments = []\n",
    "            \n",
    "            # Prepare batch\n",
    "            for idx in batch_indices:\n",
    "                prompt, sentiment = self._get_context(reviews, idx)\n",
    "                prompts.append(prompt)\n",
    "                sentiments.append(sentiment)\n",
    "            \n",
    "            # Generate texts\n",
    "            generated = self.generator(\n",
    "                prompts,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.7,\n",
    "                top_k=25,\n",
    "                repetition_penalty=1.4,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=3,\n",
    "                max_length=50,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Process generated texts\n",
    "            for idx, gen, sentiment in zip(batch_indices, generated, sentiments):\n",
    "                # The generated output is already a dictionary with 'generated_text' key\n",
    "                text = gen['generated_text']  # Removed the [0] indexing\n",
    "                cleaned_text = self._clean_generated_text(text, sentiment)\n",
    "                reconstructed[idx] = cleaned_text\n",
    "            \n",
    "        return reconstructed\n",
    "\n",
    "# Initialize reconstructor\n",
    "reconstructor = ReviewReconstructor()\n",
    "missing_percentage=0.1\n",
    "\n",
    "# Create gaps\n",
    "num_missing = int(len(final_reviews) * missing_percentage)\n",
    "missing_indices = random.sample(range(len(final_reviews)), num_missing)\n",
    "reviews_with_gaps = final_reviews.copy()\n",
    "\n",
    "for idx in missing_indices:\n",
    "    reviews_with_gaps[idx] = \"\"\n",
    "\n",
    "# Reconstruct\n",
    "reconstructed = reconstructor.reconstruct_texts(reviews_with_gaps, missing_indices)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nTotal reviews: {len(final_reviews)}\")\n",
    "print(f\"Gaps created: {num_missing}\")\n",
    "print(\"\\nReconstruction Examples:\")\n",
    "\n",
    "# Show some examples\n",
    "sample_size = min(5, len(missing_indices))\n",
    "for idx in random.sample(missing_indices, sample_size):\n",
    "    print(f\"Reconstructed: {reconstructed[idx]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4961\n",
      "           1       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.90      0.89     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:24<00:00, 1653.68 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:05<00:00, 1672.08 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.44105073331296446\n",
      "\n",
      "TinyBERT Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86      4961\n",
      "           1       0.85      0.90      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Kaggle Dataset Preprocessing \n",
    "\n",
    "# Install required packages\n",
    "#!pip install kaggle transformers scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configure Kaggle API (you'll need to upload your kaggle.json)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
    "\n",
    "# Download IMDB dataset\n",
    "#!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "# Extract dataset\n",
    "#!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
    "\n",
    "# Load and preprocess data\n",
    "#df = pd.read_csv('IMDB Dataset.csv')\n",
    "df = pd.read_csv('C:/Users/Kone/Downloads/archive/IMDB Dataset.csv')\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['review'].values, \n",
    "    df['sentiment'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Baseline Model (Logistic Regression)\n",
    "# Tokenize with basic approach\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "# Transformer Model (TinyBERT)\n",
    "# Prepare datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': X_train,\n",
    "    'label': y_train\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': X_test,\n",
    "    'label': y_test\n",
    "})\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "    batched=True\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Load TinyBERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'prajjwal1/bert-tiny',\n",
    "    num_labels=2\n",
    ")\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "bert_preds = np.array(predictions)\n",
    "print(\"\\nTinyBERT Results:\")\n",
    "print(classification_report(true_labels, bert_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing standard tokenizers:\n",
      "  Tokenizer                                             Tokens  Num_Tokens  \\\n",
      "0      BERT  [the, quick, brown, fox, jumps, over, the, laz...          22   \n",
      "1      GPT2  [The, Ġquick, Ġbrown, Ġfox, Ġjumps, Ġover, Ġth...          20   \n",
      "2   RoBERTa  [The, Ġquick, Ġbrown, Ġfox, Ġjumps, Ġover, Ġth...          20   \n",
      "\n",
      "   Vocabulary_Size  \n",
      "0            30522  \n",
      "1            50257  \n",
      "2            50265  \n",
      "\n",
      "Training custom tokenizer...\n",
      "\n",
      "Testing custom tokenizer:\n",
      "{'Original': \"This is a test of our custom tokenizer! Let's see how it performs.\", 'Encoded_IDs': [417, 162, 120, 3288, 146, 1427, 6741, 6900, 9325, 5], 'Decoded': \"ĠThis Ġis Ġa Ġtest Ġof Ġour Ġcustom Ġtoken izer ! ĠLet 's Ġsee Ġhow Ġit Ġperforms .\", 'Num_Tokens': 17}\n"
     ]
    }
   ],
   "source": [
    "# Category 2: Tokenization\n",
    "\n",
    "# Install required packages\n",
    "#!pip install transformers tokenizers datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 4. Tokenizer Comparison\n",
    "def compare_tokenizers():\n",
    "    # Initialize tokenizers\n",
    "    tokenizers = {\n",
    "        'BERT': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'GPT2': AutoTokenizer.from_pretrained('gpt2'),\n",
    "        'RoBERTa': AutoTokenizer.from_pretrained('roberta-base')\n",
    "    }\n",
    "    \n",
    "    # Sample text for comparison\n",
    "    text = \"The quick brown fox jumps over the lazy dog! Let's see how different tokenizers handle this.\"\n",
    "    \n",
    "    results = []\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        results.append({\n",
    "            'Tokenizer': name,\n",
    "            'Tokens': tokens,\n",
    "            'Num_Tokens': len(tokens),\n",
    "            'Vocabulary_Size': tokenizer.vocab_size\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 5. Custom Tokenizer Training \n",
    "def train_custom_tokenizer():\n",
    "    # Load a small dataset for training\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "    \n",
    "    # Initialize a new tokenizer (BPE model)\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    # Set up pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    \n",
    "    # Prepare training\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=25000,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # Create iterator of texts for training\n",
    "    def batch_iterator():\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            yield dataset[i:i + batch_size][\"text\"]\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(\"custom_tokenizer.json\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Test Custom Tokenizer\n",
    "def test_tokenizers(custom_tokenizer):\n",
    "    # Test text\n",
    "    test_text = \"This is a test of our custom tokenizer! Let's see how it performs.\"\n",
    "    \n",
    "    # Load saved custom tokenizer\n",
    "    custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "    \n",
    "    # Encode and decode\n",
    "    encoded = custom_tokenizer.encode(test_text)\n",
    "    decoded = custom_tokenizer.decode(encoded.ids)\n",
    "    \n",
    "    return {\n",
    "        'Original': test_text,\n",
    "        'Encoded_IDs': encoded.ids[:10],  # First 10 tokens\n",
    "        'Decoded': decoded,\n",
    "        'Num_Tokens': len(encoded.ids)\n",
    "    }\n",
    "\n",
    "# Run comparisons\n",
    "print(\"Comparing standard tokenizers:\")\n",
    "comparison_df = compare_tokenizers()\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nTraining custom tokenizer...\")\n",
    "custom_tokenizer = train_custom_tokenizer()\n",
    "\n",
    "print(\"\\nTesting custom tokenizer:\")\n",
    "test_results = test_tokenizers(custom_tokenizer)\n",
    "print(test_results)\n",
    "\n",
    "# Optional: Save custom tokenizer for reuse\n",
    "#custom_tokenizer.save_pretrained(\"./custom_tokenizer\") todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 3: Pre-trained Models \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(\n",
    "            dataset[\"text\" if \"text\" in dataset.features else \"sms\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(dataset[\"label\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, learning_rate=2e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss = {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Val Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_distilbert():\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    train_dataset = TextDataset(dataset[\"train\"], tokenizer)\n",
    "    val_dataset = TextDataset(dataset[\"test\"], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=3, device=device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def train_bert_spam():\n",
    "    dataset = load_dataset(\"sms_spam\")\n",
    "    split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    train_dataset = TextDataset(split_dataset[\"train\"], tokenizer)\n",
    "    val_dataset = TextDataset(split_dataset[\"test\"], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=1, device=device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_blip_captioning():\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    \n",
    "    def generate_caption(image_path, processor, model):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "        caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def translate_caption(caption):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        translator = AutoModelForSeq2SeqLM.from_pretrained(f\"Helsinki-NLP/opus-mt-tc-big-en-fi\")\n",
    "        translator_tokenizer = AutoTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-tc-big-en-fi\")\n",
    "        \n",
    "        translator.to(device)\n",
    "        inputs = translator_tokenizer(caption, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = translator.generate(**inputs)\n",
    "        translation = translator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return translation\n",
    "    \n",
    "    return processor, model, generate_caption, translate_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT for sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:11<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2523, Val Loss = 0.2294, Val Accuracy = 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:12<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.1404, Val Loss = 0.2250, Val Accuracy = 0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:11<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0716, Val Loss = 0.2275, Val Accuracy = 0.9295\n"
     ]
    }
   ],
   "source": [
    "print(\"Training DistilBERT for sentiment analysis...\")\n",
    "distilbert_model, distilbert_tokenizer = train_distilbert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BERT for spam classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1: 100%|██████████| 279/279 [01:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0768, Val Loss = 0.0499, Val Accuracy = 0.9883\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining BERT for spam classification...\")\n",
    "bert_model, bert_tokenizer = train_bert_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up BLIP for image captioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kone\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-tc-big-en-fi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: image-1.jpg\n",
      "Caption: a bowl of oranges with a half of a grape\n",
      "Translation: kulhollinen appelsiineja, joissa on puolikas rypälettä\n",
      "\n",
      "Image: image-2.jpg\n",
      "Caption: the tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori\n",
      "Translation: tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori\n",
      "\n",
      "Image: image-3.jpg\n",
      "Caption: a fluffy orange cat with a white face\n",
      "Translation: pörröinen oranssi kissa, jolla on valkoiset kasvot\n",
      "\n",
      "Image: image-4.jpg\n",
      "Caption: the old bridge in mostar, bosnia\n",
      "Translation: Mostarissa sijaitseva vanha silta, bosnia\n",
      "\n",
      "Image: image-5.jpg\n",
      "Caption: a river with trees and bushes in the background\n",
      "Translation: joki, jonka taustalla on puita ja pensaita\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up BLIP for image captioning...\")\n",
    "blip_processor, blip_model, caption_fn, translate_fn = setup_blip_captioning()\n",
    "\n",
    "\n",
    "\n",
    "# Test BLIP with sample images\n",
    "sample_images = [\"image-1.jpg\", \"image-2.jpg\", \"image-3.jpg\", \"image-4.jpg\", \"image-5.jpg\"]\n",
    "for img_path in sample_images:\n",
    "    try:\n",
    "        caption = caption_fn(img_path, blip_processor, blip_model)\n",
    "        translation = translate_fn(caption)\n",
    "        print(f\"\\nImage: {img_path}\")\n",
    "        print(f\"Caption: {caption}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image {img_path} not found. Please provide valid image paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 14:38:10,448\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 16.0, 'object_store_memory': 4922846822.0, 'node:127.0.0.1': 1.0, 'memory': 9845693646.0, 'GPU': 1.0}\n",
      "True\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(ray\u001b[38;5;241m.\u001b[39mcluster_resources())\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTuneReportCallback\u001b[39;00m(\u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39mTrainerCallback):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, state, control, metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metrics:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "# Get the directory where the notebook is located\n",
    "notebook_dir = os.path.dirname(os.path.abspath('C:/Users/Kone/ftllm/wk02/ex02.ipynb'))\n",
    "\n",
    "# Create paths relative to the notebook directory\n",
    "results_dir = os.path.join(notebook_dir, 'ray_tune_results')\n",
    "logs_dir = os.path.join(notebook_dir, 'ray_logs')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Initialize ray\n",
    "ray.shutdown()\n",
    "ray.init(num_gpus=1)\n",
    "print(ray.cluster_resources())\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "class TuneReportCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if metrics:\n",
    "            tune.report(\n",
    "                eval_accuracy=metrics.get(\"eval_accuracy\", 0),\n",
    "                eval_loss=metrics.get(\"eval_loss\", 0)\n",
    "            )\n",
    "\n",
    "class TuneTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.add_callback(TuneReportCallback())  # Add proper callback instance\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        if metrics:\n",
    "            tune.report(\n",
    "                eval_accuracy=metrics.get(\"eval_accuracy\", 0),\n",
    "                eval_loss=metrics.get(\"eval_loss\", 0)\n",
    "            )\n",
    "        return super().on_evaluate(args, state, control, metrics, **kwargs)\n",
    "    \n",
    "def model_init():\n",
    "    \"\"\"Initialize a new model for each trial\"\"\"\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "\n",
    "def hp_space(trial):\n",
    "    \"\"\"Define the hyperparameter search space\"\"\"\n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "        \"per_device_train_batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"per_device_eval_batch_size\": tune.choice([16, 32, 64]),\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"warmup_steps\": tune.choice([100, 200, 300]),\n",
    "        \"dropout\": tune.uniform(0.1, 0.5)\n",
    "    }\n",
    "\n",
    "def trial_name_creator(trial):\n",
    "    \"\"\"Create unique name for each trial with hyperparams\"\"\"\n",
    "    return f\"trial_{trial.trial_id}-lr_{trial.config['learning_rate']:.2e}-bs_{trial.config['per_device_train_batch_size']}\"\n",
    "\n",
    "def train_with_ray_tune(num_trials=20):\n",
    "    # Load dataset (using IMDB for example)\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        encoding = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256  # Reduced from 512 for speed\n",
    "        )\n",
    "        encoding[\"labels\"] = examples[\"label\"]\n",
    "        return encoding\n",
    "    \n",
    "    # Prepare dataset\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    # Use subset of test set for faster evaluation during tuning\n",
    "    eval_dataset = tokenized_datasets[\"test\"].select(range(len(tokenized_datasets[\"test\"]) // 5))\n",
    "    \n",
    "    # Define training arguments template\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=results_dir,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "        save_strategy=\"no\",\n",
    "        logging_dir=logs_dir,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=1,\n",
    "        dataloader_num_workers=2,\n",
    "        logging_steps=500,\n",
    "        # Add these to ensure evaluation happens\n",
    "        do_eval=True\n",
    "    )\n",
    "    \n",
    "    # Define ASHA scheduler\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=3,\n",
    "        grace_period=1,\n",
    "        reduction_factor=3,  # Increased for faster pruning\n",
    "        brackets=1\n",
    "    )\n",
    "    \n",
    "    def train_func(config):\n",
    "        # Update training arguments with trial config\n",
    "        for key, value in config.items():\n",
    "            if hasattr(training_args, key):\n",
    "                setattr(training_args, key, value)\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = TuneTrainer(\n",
    "            model_init=model_init,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=eval_dataset\n",
    "        )\n",
    "        \n",
    "        # Train and evaluate - removed manual callback addition\n",
    "        trainer.train()\n",
    "\n",
    "\n",
    "    print(\"Starting Ray Tune optimization...\")\n",
    "    analysis = tune.run(\n",
    "        train_func,\n",
    "        config=hp_space(None),\n",
    "        storage_path=\"C:/ray_temp\",\n",
    "        trial_dirname_creator=trial_name_creator,\n",
    "        name=\"tune_bert\",\n",
    "        num_samples=num_trials,\n",
    "        scheduler=scheduler,\n",
    "        metric=\"eval_accuracy\",  # This should match the metric name in tune.report()\n",
    "        mode=\"max\",\n",
    "        resources_per_trial={\"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "        progress_reporter=tune.CLIReporter(\n",
    "            parameter_columns=[\"learning_rate\", \"per_device_train_batch_size\", \"warmup_steps\", \"dropout\"],\n",
    "            metric_columns=[\"eval_accuracy\", \"eval_loss\", \"training_iteration\"],\n",
    "            max_report_frequency=120\n",
    "        )\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Get best trial\n",
    "    best_trial = analysis.get_best_trial(\"eval_accuracy\", \"max\", \"last\")\n",
    "    print(\"\\nBest trial config:\", best_trial.config)\n",
    "    print(\"Best trial final accuracy:\", best_trial.last_result[\"eval_accuracy\"])\n",
    "    \n",
    "    # Final evaluation with full test set\n",
    "    print(\"\\nPerforming final evaluation with full test set...\")\n",
    "    final_trainer = TuneTrainer(\n",
    "        model_init=model_init,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"]  # Full test set\n",
    "    )\n",
    "    \n",
    "    # Set the best hyperparameters\n",
    "    for key, value in best_trial.config.items():\n",
    "        if hasattr(training_args, key):\n",
    "            setattr(training_args, key, value)\n",
    "    \n",
    "    final_metrics = final_trainer.evaluate()\n",
    "    print(\"\\nFinal evaluation metrics on full test set:\", final_metrics)\n",
    "    \n",
    "    return best_trial.config\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    best_config = train_with_ray_tune(num_trials=20)\n",
    "\n",
    "    # Save best configuration\n",
    "    print(\"\\nSaving best configuration...\")\n",
    "    with open(\"best_hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(best_config, f)\n",
    "\n",
    "    print(\"\\nHyperparameter optimization complete!\")\n",
    "    print(\"Best configuration saved to best_hyperparameters.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
