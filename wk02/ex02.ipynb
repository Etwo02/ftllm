{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLPAug imported successfully\n",
      "Torch version: 2.5.1+cu121\n",
      "Transformers version: 4.48.1\n",
      "CUDA available: True\n",
      "Current device: NVIDIA GeForce RTX 4080 SUPER\n",
      "cuda\n",
      "Original reviews: 500\n",
      "Augmented reviews: 1362\n",
      "Total reviews: 1862\n",
      "\n",
      "Example augmentations:\n",
      "\n",
      "Original: Outstanding performance.\n",
      "Augmented: not what i originally expected at all.\n",
      "\n",
      "Original: Honestly, It's the best I've ever used.\n",
      "Augmented: fortunately, great customer service.\n",
      "\n",
      "Original: Honestly, Exactly what I was looking for.\n",
      "Augmented: Honestly, This is amazing!\n"
     ]
    }
   ],
   "source": [
    "# Category 1, Dataset Preparation \n",
    "\n",
    "# 1. Synthetic Dataset Creation and Augmentation \n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import nlpaug.augmenter.word as naw\n",
    "import torch\n",
    "print(\"NLPAug imported successfully\")\n",
    "import random\n",
    "\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Current device: {torch.cuda.get_device_name(0)}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Sample positive and negative reviews\n",
    "positive_reviews = [\n",
    "    \"This product is amazing!\",\n",
    "    \"I highly recommend this.\",\n",
    "    \"It's the best I've ever used.\",\n",
    "    \"Excellent quality and value.\",\n",
    "    \"Five stars!\",\n",
    "    \"Great customer service.\",\n",
    "    \"Exactly what I was looking for.\",\n",
    "    \"Very satisfied with my purchase.\",\n",
    "    \"Outstanding performance.\",\n",
    "    \"Worth every penny!\"\n",
    "]\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This product is terrible.\",\n",
    "    \"I would not recommend this.\",\n",
    "    \"It's the worst I've ever used.\",\n",
    "    \"Poor quality and overpriced.\",\n",
    "    \"One star!\",\n",
    "    \"Horrible customer service.\",\n",
    "    \"Not what I expected at all.\",\n",
    "    \"Very disappointed with my purchase.\",\n",
    "    \"Unreliable performance.\",\n",
    "    \"Complete waste of money!\"\n",
    "]\n",
    "\n",
    "# Generate 500 base sentences by repeating and slightly modifying the samples\n",
    "all_reviews = []\n",
    "for i in range(250):\n",
    "    # Add some random variation to avoid exact duplicates\n",
    "    pos_review = positive_reviews[i % len(positive_reviews)]\n",
    "    neg_review = negative_reviews[i % len(negative_reviews)]\n",
    "\n",
    "    # Add simple variations to make the dataset more diverse\n",
    "    if random.random() > 0.5:\n",
    "        pos_review = \"Honestly, \" + pos_review\n",
    "    if random.random() > 0.5:\n",
    "        neg_review = \"Unfortunately, \" + neg_review\n",
    "\n",
    "    all_reviews.append(pos_review)\n",
    "    all_reviews.append(neg_review)\n",
    "\n",
    "# Initialize augmenters\n",
    "aug_insert = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"insert\",\n",
    "    aug_p=0.1  # Probability of augmenting each word\n",
    ")\n",
    "\n",
    "aug_sub = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased',\n",
    "    action=\"substitute\",\n",
    "    aug_p=0.1,\n",
    "    stopwords=['not', 'no', 'never']  # Prevent changing sentiment-critical words\n",
    ")\n",
    "\n",
    "\n",
    "# Augment the dataset\n",
    "augmented_reviews = []\n",
    "for review in all_reviews:\n",
    "    try:\n",
    "        # Insert words\n",
    "        aug_text = aug_insert.augment(review)[0]\n",
    "        augmented_reviews.append(aug_text)\n",
    "\n",
    "        # Substitute words\n",
    "        aug_text = aug_sub.augment(review)[0]\n",
    "        augmented_reviews.append(aug_text)\n",
    "\n",
    "        # Simple word deletion (manual approach)\n",
    "        words = review.split()\n",
    "        if len(words) > 3:  # Only delete if we have enough words\n",
    "            del_idx = random.randint(0, len(words)-1)\n",
    "            words.pop(del_idx)\n",
    "            aug_text = \" \".join(words)\n",
    "            augmented_reviews.append(aug_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting review: {review}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Combine original and augmented reviews\n",
    "final_reviews = all_reviews + augmented_reviews\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Original reviews: {len(all_reviews)}\")\n",
    "print(f\"Augmented reviews: {len(augmented_reviews)}\")\n",
    "print(f\"Total reviews: {len(final_reviews)}\")\n",
    "\n",
    "# Print some examples\n",
    "print(\"\\nExample augmentations:\")\n",
    "for i in range(3):\n",
    "    orig_idx = random.randint(0, len(all_reviews)-1)\n",
    "    aug_idx = random.randint(0, len(augmented_reviews)-1)\n",
    "    print(f\"\\nOriginal: {all_reviews[orig_idx]}\")\n",
    "    print(f\"Augmented: {augmented_reviews[aug_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 6/6 [00:07<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total reviews: 1867\n",
      "Gaps created: 186\n",
      "\n",
      "Reconstruction Examples:\n",
      "Reconstructed: a total  this product is just terrible. this product screams terrible.\n",
      "Reconstructed: review poor work and overpriced. Poor and over priced.\n",
      "Reconstructed: this product line is terrible. this product appeared terrible.  honestly, i personally recommend this.\n",
      "Reconstructed: Sadly, Very disappointed with my purchase.  Unfortunately, Unreliable performance.\n",
      "Reconstructed: a complete  Unfortunately, Not what I expected at all.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Handling Missing Values\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ReviewReconstructor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.generator = self._create_generator()\n",
    "        # Sentiment keywords for better context understanding\n",
    "        self.positive_keywords = {'amazing', 'recommend', 'best', 'excellent', 'stars', 'great', \n",
    "                                'exactly', 'satisfied', 'outstanding', 'worth'}\n",
    "        self.negative_keywords = {'terrible', 'not', 'worst', 'poor', 'horrible', 'disappointed', \n",
    "                                'unreliable', 'waste', 'unfortunately'}\n",
    "\n",
    "    def _create_generator(self):\n",
    "        return pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=\"t5-base\",\n",
    "            device=0 if self.device == 'cuda' else -1,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "    def _detect_sentiment(self, text):\n",
    "        \"\"\"Detect sentiment based on keyword presence.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        pos_count = sum(1 for word in self.positive_keywords if word in text_lower)\n",
    "        neg_count = sum(1 for word in self.negative_keywords if word in text_lower)\n",
    "        return 'positive' if pos_count > neg_count else 'negative'\n",
    "\n",
    "    def _get_context(self, text_list, current_idx):\n",
    "        # Get surrounding context\n",
    "        prev_texts = [t for t in text_list[max(0, current_idx - 2):current_idx] if t.strip()]\n",
    "        next_texts = [t for t in text_list[current_idx + 1:current_idx + 3] if t.strip()]\n",
    "        \n",
    "        # Combine context\n",
    "        context_text = \" \".join(prev_texts + next_texts)\n",
    "        sentiment = self._detect_sentiment(context_text)\n",
    "        \n",
    "        # Format prompt with sentiment guidance\n",
    "        prompt = f\"complete {sentiment} review:\"\n",
    "        if prev_texts:\n",
    "            prompt += f\" {' '.join(prev_texts)}\"\n",
    "        prompt += \" [MISSING]\"\n",
    "        if next_texts:\n",
    "            prompt += f\" {' '.join(next_texts)}\"\n",
    "            \n",
    "        return prompt, sentiment\n",
    "\n",
    "    def _clean_generated_text(self, text, sentiment):\n",
    "        \"\"\"Clean and validate generated text.\"\"\"\n",
    "        # Remove common prefix artifacts\n",
    "        artifacts = [\n",
    "            \"review::\", \"negative review:\", \"complete positive review:\",\n",
    "            \"complete negative review:\", \"positive review:\", \"complete review:\", \":\", \"True\"\n",
    "        ]\n",
    "        for artifact in artifacts:\n",
    "            text = text.replace(artifact, \"\").strip()\n",
    "\n",
    "        # Remove [MISSING] placeholders\n",
    "        text = text.replace(\"[MISSING]\", \"\").strip()\n",
    "\n",
    "        # Ensure proper sentence structure\n",
    "        if len(text.split()) < 3:\n",
    "            text = \"This product is excellent!\" if sentiment == 'positive' else \"This product is disappointing.\"\n",
    "\n",
    "        # Ensure proper ending punctuation\n",
    "        if not any(text.endswith(char) for char in \".!?\"):\n",
    "            text += \".\"\n",
    "\n",
    "        return text\n",
    "\n",
    "    def reconstruct_texts(self, reviews, missing_indices, batch_size=32):\n",
    "        \"\"\"Reconstruct missing texts with batched processing.\"\"\"\n",
    "        reconstructed = reviews.copy()\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(missing_indices), batch_size)):\n",
    "            batch_indices = missing_indices[i:i + batch_size]\n",
    "            prompts = []\n",
    "            sentiments = []\n",
    "            \n",
    "            # Prepare batch\n",
    "            for idx in batch_indices:\n",
    "                prompt, sentiment = self._get_context(reviews, idx)\n",
    "                prompts.append(prompt)\n",
    "                sentiments.append(sentiment)\n",
    "            \n",
    "            # Generate texts\n",
    "            generated = self.generator(\n",
    "                prompts,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.7,\n",
    "                top_k=25,\n",
    "                repetition_penalty=1.4,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=3,\n",
    "                max_length=50,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            # Process generated texts\n",
    "            for idx, gen, sentiment in zip(batch_indices, generated, sentiments):\n",
    "                # The generated output is already a dictionary with 'generated_text' key\n",
    "                text = gen['generated_text']  # Removed the [0] indexing\n",
    "                cleaned_text = self._clean_generated_text(text, sentiment)\n",
    "                reconstructed[idx] = cleaned_text\n",
    "            \n",
    "        return reconstructed\n",
    "\n",
    "# Initialize reconstructor\n",
    "reconstructor = ReviewReconstructor()\n",
    "missing_percentage=0.1\n",
    "\n",
    "# Create gaps\n",
    "num_missing = int(len(final_reviews) * missing_percentage)\n",
    "missing_indices = random.sample(range(len(final_reviews)), num_missing)\n",
    "reviews_with_gaps = final_reviews.copy()\n",
    "\n",
    "for idx in missing_indices:\n",
    "    reviews_with_gaps[idx] = \"\"\n",
    "\n",
    "# Reconstruct\n",
    "reconstructed = reconstructor.reconstruct_texts(reviews_with_gaps, missing_indices)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nTotal reviews: {len(final_reviews)}\")\n",
    "print(f\"Gaps created: {num_missing}\")\n",
    "print(\"\\nReconstruction Examples:\")\n",
    "\n",
    "# Show some examples\n",
    "sample_size = min(5, len(missing_indices))\n",
    "for idx in random.sample(missing_indices, sample_size):\n",
    "    print(f\"Reconstructed: {reconstructed[idx]}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      4961\n",
      "           1       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.90      0.89     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 40000/40000 [00:24<00:00, 1653.68 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:05<00:00, 1672.08 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.44105073331296446\n",
      "\n",
      "TinyBERT Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86      4961\n",
      "           1       0.85      0.90      0.87      5039\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Kaggle Dataset Preprocessing \n",
    "\n",
    "# Install required packages\n",
    "#!pip install kaggle transformers scikit-learn pandas numpy\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configure Kaggle API (you'll need to upload your kaggle.json)\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
    "\n",
    "# Download IMDB dataset\n",
    "#!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "# Extract dataset\n",
    "#!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
    "\n",
    "# Load and preprocess data\n",
    "#df = pd.read_csv('IMDB Dataset.csv')\n",
    "df = pd.read_csv('C:/Users/Kone/Downloads/archive/IMDB Dataset.csv')\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['review'].values, \n",
    "    df['sentiment'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Baseline Model (Logistic Regression)\n",
    "# Tokenize with basic approach\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "lr_preds = lr_model.predict(X_test_tfidf)\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(classification_report(y_test, lr_preds))\n",
    "\n",
    "# Transformer Model (TinyBERT)\n",
    "# Prepare datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': X_train,\n",
    "    'label': y_train\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': X_test,\n",
    "    'label': y_test\n",
    "})\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "    batched=True\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda x: tokenizer(x['text'], padding=True, truncation=True),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Load TinyBERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'prajjwal1/bert-tiny',\n",
    "    num_labels=2\n",
    ")\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "bert_preds = np.array(predictions)\n",
    "print(\"\\nTinyBERT Results:\")\n",
    "print(classification_report(true_labels, bert_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing standard tokenizers:\n",
      "  Tokenizer                                             Tokens  Num_Tokens  \\\n",
      "0      BERT  [the, quick, brown, fox, jumps, over, the, laz...          22   \n",
      "1      GPT2  [The, Ġquick, Ġbrown, Ġfox, Ġjumps, Ġover, Ġth...          20   \n",
      "2   RoBERTa  [The, Ġquick, Ġbrown, Ġfox, Ġjumps, Ġover, Ġth...          20   \n",
      "\n",
      "   Vocabulary_Size  \n",
      "0            30522  \n",
      "1            50257  \n",
      "2            50265  \n",
      "\n",
      "Training custom tokenizer...\n",
      "\n",
      "Testing custom tokenizer:\n",
      "{'Original': \"This is a test of our custom tokenizer! Let's see how it performs.\", 'Encoded_IDs': [417, 162, 120, 3288, 146, 1427, 6741, 6900, 9325, 5], 'Decoded': \"ĠThis Ġis Ġa Ġtest Ġof Ġour Ġcustom Ġtoken izer ! ĠLet 's Ġsee Ġhow Ġit Ġperforms .\", 'Num_Tokens': 17}\n"
     ]
    }
   ],
   "source": [
    "# Category 2: Tokenization\n",
    "\n",
    "# Install required packages\n",
    "#!pip install transformers tokenizers datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 4. Tokenizer Comparison\n",
    "def compare_tokenizers():\n",
    "    # Initialize tokenizers\n",
    "    tokenizers = {\n",
    "        'BERT': AutoTokenizer.from_pretrained('bert-base-uncased'),\n",
    "        'GPT2': AutoTokenizer.from_pretrained('gpt2'),\n",
    "        'RoBERTa': AutoTokenizer.from_pretrained('roberta-base')\n",
    "    }\n",
    "    \n",
    "    # Sample text for comparison\n",
    "    text = \"The quick brown fox jumps over the lazy dog! Let's see how different tokenizers handle this.\"\n",
    "    \n",
    "    results = []\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        results.append({\n",
    "            'Tokenizer': name,\n",
    "            'Tokens': tokens,\n",
    "            'Num_Tokens': len(tokens),\n",
    "            'Vocabulary_Size': tokenizer.vocab_size\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 5. Custom Tokenizer Training \n",
    "def train_custom_tokenizer():\n",
    "    # Load a small dataset for training\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "    \n",
    "    # Initialize a new tokenizer (BPE model)\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    \n",
    "    # Set up pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    \n",
    "    # Prepare training\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=25000,\n",
    "        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    )\n",
    "    \n",
    "    # Create iterator of texts for training\n",
    "    def batch_iterator():\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(dataset), batch_size):\n",
    "            yield dataset[i:i + batch_size][\"text\"]\n",
    "    \n",
    "    # Train tokenizer\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save(\"custom_tokenizer.json\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Test Custom Tokenizer\n",
    "def test_tokenizers(custom_tokenizer):\n",
    "    # Test text\n",
    "    test_text = \"This is a test of our custom tokenizer! Let's see how it performs.\"\n",
    "    \n",
    "    # Load saved custom tokenizer\n",
    "    custom_tokenizer = Tokenizer.from_file(\"custom_tokenizer.json\")\n",
    "    \n",
    "    # Encode and decode\n",
    "    encoded = custom_tokenizer.encode(test_text)\n",
    "    decoded = custom_tokenizer.decode(encoded.ids)\n",
    "    \n",
    "    return {\n",
    "        'Original': test_text,\n",
    "        'Encoded_IDs': encoded.ids[:10],  # First 10 tokens\n",
    "        'Decoded': decoded,\n",
    "        'Num_Tokens': len(encoded.ids)\n",
    "    }\n",
    "\n",
    "# Run comparisons\n",
    "print(\"Comparing standard tokenizers:\")\n",
    "comparison_df = compare_tokenizers()\n",
    "print(comparison_df)\n",
    "\n",
    "print(\"\\nTraining custom tokenizer...\")\n",
    "custom_tokenizer = train_custom_tokenizer()\n",
    "\n",
    "print(\"\\nTesting custom tokenizer:\")\n",
    "test_results = test_tokenizers(custom_tokenizer)\n",
    "print(test_results)\n",
    "\n",
    "# Optional: Save custom tokenizer for reuse\n",
    "#custom_tokenizer.save_pretrained(\"./custom_tokenizer\") todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category 3: Pre-trained Models \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(\n",
    "            dataset[\"text\" if \"text\" in dataset.features else \"sms\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        self.labels = torch.tensor(dataset[\"label\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, learning_rate=2e-5):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss = {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Val Accuracy = {accuracy:.4f}\")\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pt\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_distilbert():\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    train_dataset = TextDataset(dataset[\"train\"], tokenizer)\n",
    "    val_dataset = TextDataset(dataset[\"test\"], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=3, device=device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def train_bert_spam():\n",
    "    dataset = load_dataset(\"sms_spam\")\n",
    "    split_dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    train_dataset = TextDataset(split_dataset[\"train\"], tokenizer)\n",
    "    val_dataset = TextDataset(split_dataset[\"test\"], tokenizer)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = train_model(model, train_loader, val_loader, num_epochs=1, device=device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def setup_blip_captioning():\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    \n",
    "    def generate_caption(image_path, processor, model):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs)\n",
    "        caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return caption\n",
    "    \n",
    "    def translate_caption(caption):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        translator = AutoModelForSeq2SeqLM.from_pretrained(f\"Helsinki-NLP/opus-mt-tc-big-en-fi\")\n",
    "        translator_tokenizer = AutoTokenizer.from_pretrained(f\"Helsinki-NLP/opus-mt-tc-big-en-fi\")\n",
    "        \n",
    "        translator.to(device)\n",
    "        inputs = translator_tokenizer(caption, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = translator.generate(**inputs)\n",
    "        translation = translator_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return translation\n",
    "    \n",
    "    return processor, model, generate_caption, translate_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DistilBERT for sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/3: 100%|██████████| 1563/1563 [03:11<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.2523, Val Loss = 0.2294, Val Accuracy = 0.9048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 1563/1563 [03:12<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.1404, Val Loss = 0.2250, Val Accuracy = 0.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 1563/1563 [03:11<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0716, Val Loss = 0.2275, Val Accuracy = 0.9295\n"
     ]
    }
   ],
   "source": [
    "print(\"Training DistilBERT for sentiment analysis...\")\n",
    "distilbert_model, distilbert_tokenizer = train_distilbert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training BERT for spam classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/1: 100%|██████████| 279/279 [01:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0768, Val Loss = 0.0499, Val Accuracy = 0.9883\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining BERT for spam classification...\")\n",
    "bert_model, bert_tokenizer = train_bert_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up BLIP for image captioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kone\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-tc-big-en-fi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image: image-1.jpg\n",
      "Caption: a bowl of oranges with a half of a grape\n",
      "Translation: kulhollinen appelsiineja, joissa on puolikas rypälettä\n",
      "\n",
      "Image: image-2.jpg\n",
      "Caption: the tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori\n",
      "Translation: tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori tori\n",
      "\n",
      "Image: image-3.jpg\n",
      "Caption: a fluffy orange cat with a white face\n",
      "Translation: pörröinen oranssi kissa, jolla on valkoiset kasvot\n",
      "\n",
      "Image: image-4.jpg\n",
      "Caption: the old bridge in mostar, bosnia\n",
      "Translation: Mostarissa sijaitseva vanha silta, bosnia\n",
      "\n",
      "Image: image-5.jpg\n",
      "Caption: a river with trees and bushes in the background\n",
      "Translation: joki, jonka taustalla on puita ja pensaita\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSetting up BLIP for image captioning...\")\n",
    "blip_processor, blip_model, caption_fn, translate_fn = setup_blip_captioning()\n",
    "\n",
    "\n",
    "\n",
    "# Test BLIP with sample images\n",
    "sample_images = [\"image-1.jpg\", \"image-2.jpg\", \"image-3.jpg\", \"image-4.jpg\", \"image-5.jpg\"]\n",
    "for img_path in sample_images:\n",
    "    try:\n",
    "        caption = caption_fn(img_path, blip_processor, blip_model)\n",
    "        translation = translate_fn(caption)\n",
    "        print(f\"\\nImage: {img_path}\")\n",
    "        print(f\"Caption: {caption}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image {img_path} not found. Please provide valid image paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Loading tokenizer...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [00:06<00:00, 4070.87 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [00:06<00:00, 3991.68 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:12<00:00, 3882.89 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Category 4: Hyperparameter Tuning\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"Prepare the dataset without caching.\"\"\"\n",
    "    print(\"Loading IMDB dataset...\")\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    print(\"Tokenizing dataset...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=False  # Ensure no caching\n",
    "    )\n",
    "\n",
    "    return tokenized_datasets\n",
    "\n",
    "# Prepare the dataset\n",
    "tokenized_datasets = prepare_dataset()\n",
    "print(\"Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 12:55:59,855\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "DeprecationWarning",
     "evalue": "The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDeprecationWarning\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 114\u001b[0m\n\u001b[0;32m    105\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ASHAScheduler(\n\u001b[0;32m    106\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    107\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     reduction_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert_tune\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increase verbosity for debugging\u001b[39;49;00m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest hyperparameters found:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mbest_config)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest metrics achieved:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis\u001b[38;5;241m.\u001b[39mbest_result_metrics)\n",
      "File \u001b[1;32mc:\\Users\\Kone\\ftllm\\yes\\Lib\\site-packages\\ray\\tune\\tune.py:587\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, storage_filesystem, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, resume, resume_config, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, chdir_to_trial_dir, local_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(\n\u001b[0;32m    583\u001b[0m         ENV_VAR_DEPRECATION_MESSAGE\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_AIR_LOCAL_CACHE_DIR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    584\u001b[0m     )\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(\n\u001b[0;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `local_dir` argument is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    589\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should set the `storage_path` instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the docs: https://docs.ray.io/en/latest/train/user-guides/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    591\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersistent-storage.html#setting-the-local-staging-directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    592\u001b[0m     )\n\u001b[0;32m    594\u001b[0m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39musage\u001b[38;5;241m.\u001b[39musage_lib\u001b[38;5;241m.\u001b[39mrecord_library_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtune\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;66;03m# Tracking environment variable usage here will also catch:\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;66;03m# 1.) Tuner.fit() usage\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# 2.) Trainer.fit() usage\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# 3.) Ray client usage (env variables are inherited by the Ray runtime env)\u001b[39;00m\n",
      "\u001b[1;31mDeprecationWarning\u001b[0m: The `local_dir` argument is deprecated. You should set the `storage_path` instead. See the docs: https://docs.ray.io/en/latest/train/user-guides/persistent-storage.html#setting-the-local-staging-directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import tempfile\n",
    "\n",
    "# Create temp directory with shorter path\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"ray_\")\n",
    "BASE_DIR = temp_dir\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "LOG_DIR = os.path.join(BASE_DIR, \"logs\")\n",
    "\n",
    "# Create all necessary directories\n",
    "for dir_path in [BASE_DIR, OUTPUT_DIR, LOG_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    os.chmod(dir_path, 0o777)  # Ensure write permissions\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, \n",
    "        predictions, \n",
    "        average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(local_mode=True)  # Run in local mode for debugging\n",
    "\n",
    "def train_func(config):\n",
    "    # Create unique directory for this trial\n",
    "    trial_dir = os.path.join(OUTPUT_DIR, tune.get_trial_name())\n",
    "    os.makedirs(trial_dir, exist_ok=True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2,\n",
    "    )\n",
    "    \n",
    "    # Set dropout\n",
    "    model.config.hidden_dropout_prob = config[\"dropout\"]\n",
    "    model.config.attention_probs_dropout_prob = config[\"dropout\"]\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=trial_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"],\n",
    "        num_train_epochs=1,\n",
    "        logging_dir=os.path.join(trial_dir, \"logs\"),\n",
    "        # Disable some features to reduce filesystem operations\n",
    "        report_to=[],  # Disable TensorBoard logging\n",
    "        logging_steps=50,\n",
    "        save_total_limit=None,\n",
    "        # Training parameters\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        # Reduce memory usage\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_accumulation_steps=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"].select(range(1000)),\n",
    "        eval_dataset=tokenized_datasets[\"test\"].select(range(1000)),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    tune.report(\n",
    "        accuracy=metrics[\"eval_accuracy\"],\n",
    "        precision=metrics[\"eval_precision\"],\n",
    "        recall=metrics[\"eval_recall\"],\n",
    "        f1=metrics[\"eval_f1\"]\n",
    "    )\n",
    "\n",
    "# Simplified config space\n",
    "config = {\n",
    "    \"learning_rate\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"batch_size\": tune.choice([16, 32]),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"warmup_steps\": tune.choice([100, 200])\n",
    "}\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"accuracy\",\n",
    "    mode=\"max\",\n",
    "    max_t=1,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "try:\n",
    "    analysis = tune.run(\n",
    "        train_func,\n",
    "        config=config,\n",
    "        num_samples=2,\n",
    "        scheduler=scheduler,\n",
    "        storage_path=BASE_DIR,\n",
    "        name=\"bert_tune\",\n",
    "        resources_per_trial={\"gpu\": 1 if torch.cuda.is_available() else 0},\n",
    "        verbose=1  # Increase verbosity for debugging\n",
    "    )\n",
    "\n",
    "    print(\"\\nBest hyperparameters found:\", analysis.best_config)\n",
    "    print(\"\\nBest metrics achieved:\", analysis.best_result_metrics)\n",
    "\n",
    "finally:\n",
    "    ray.shutdown()\n",
    "    # Cleanup\n",
    "    try:\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
